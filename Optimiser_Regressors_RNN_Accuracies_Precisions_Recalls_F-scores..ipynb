{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7590bb33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17000 images belonging to 10 classes.\n",
      "Found 2999 images belonging to 10 classes.\n",
      "Epoch 1/5\n",
      "266/266 [==============================] - 33s 119ms/step - loss: 1.1123 - accuracy: 0.6334 - val_loss: 0.3322 - val_accuracy: 0.8813\n",
      "Epoch 2/5\n",
      "266/266 [==============================] - 32s 120ms/step - loss: 0.4920 - accuracy: 0.8441 - val_loss: 0.1721 - val_accuracy: 0.9453\n",
      "Epoch 3/5\n",
      "266/266 [==============================] - 32s 121ms/step - loss: 0.3583 - accuracy: 0.8872 - val_loss: 0.1049 - val_accuracy: 0.9697\n",
      "Epoch 4/5\n",
      "266/266 [==============================] - 30s 111ms/step - loss: 0.2960 - accuracy: 0.9084 - val_loss: 0.0856 - val_accuracy: 0.9740\n",
      "Epoch 5/5\n",
      "266/266 [==============================] - 32s 119ms/step - loss: 0.2581 - accuracy: 0.9192 - val_loss: 0.0710 - val_accuracy: 0.9787\n",
      "47/47 - 2s - loss: 0.0710 - accuracy: 0.9787 - 2s/epoch - 48ms/step\n",
      "Test accuracy: 0.9786595702171326\n"
     ]
    }
   ],
   "source": [
    "# import necessary libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define the paths to the train and test folders on your desktop\n",
    "train_path = 'C:/Users/chand/Desktop/New folder/DeepLearning/Dataset/train'\n",
    "test_path = 'C:/Users/chand/Desktop/New folder/DeepLearning/Dataset/test'\n",
    "\n",
    "# define image and batch size\n",
    "img_height, img_width = 28, 28\n",
    "batch_size = 64\n",
    "\n",
    "# create ImageDataGenerator objects for data augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255.0,\n",
    "    rotation_range=20,\n",
    "    zoom_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=False,\n",
    "    vertical_flip=False,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255.0\n",
    ")\n",
    "\n",
    "# create flow_from_directory generators for train and test datasets\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    directory=train_path,\n",
    "    target_size=(img_height, img_width),\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    directory=test_path,\n",
    "    target_size=(img_height, img_width),\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=False,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# define the model architecture\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(img_height, img_width, 1)),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation=\"relu\"),\n",
    "        layers.Dense(train_generator.num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# train the model\n",
    "# MODEL.summary\n",
    "\n",
    "history = model.fit(train_generator, epochs=5, validation_data=test_generator)\n",
    "\n",
    "# evaluate the model on test set\n",
    "test_loss, test_acc = model.evaluate(test_generator, verbose=2)\n",
    "print(\"Test accuracy:\", test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "996f9250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 2s 46ms/step\n",
      "Optimizer: SGD\n",
      "Test loss: 0.3450772762298584\n",
      "Test accuracy: 0.8989663124084473\n",
      "Precision: 0.9867938\n",
      "Recall: 0.9962963\n",
      "F-score: 0.99152225\n",
      "\n",
      "\n",
      "47/47 [==============================] - 3s 53ms/step\n",
      "Optimizer: Adam\n",
      "Test loss: 0.05208950489759445\n",
      "Test accuracy: 0.9833277463912964\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F-score: 1.0\n",
      "\n",
      "\n",
      "47/47 [==============================] - 2s 49ms/step\n",
      "Optimizer: Adadelta\n",
      "Test loss: 1.9836329221725464\n",
      "Test accuracy: 0.5055018067359924\n",
      "Precision: 0.9133378\n",
      "Recall: 0.99925923\n",
      "F-score: 0.9543686\n",
      "\n",
      "\n",
      "47/47 [==============================] - 2s 44ms/step\n",
      "Optimizer: RMSprop\n",
      "Test loss: 0.04784171283245087\n",
      "Test accuracy: 0.9869956374168396\n",
      "Precision: 0.9988901\n",
      "Recall: 1.0\n",
      "F-score: 0.9994447\n",
      "\n",
      "\n",
      "47/47 [==============================] - 2s 45ms/step\n",
      "Optimizer: Adagrad\n",
      "Test loss: 0.8353080749511719\n",
      "Test accuracy: 0.811603844165802\n",
      "Precision: 0.99924815\n",
      "Recall: 0.98444444\n",
      "F-score: 0.991791\n",
      "\n",
      "\n",
      "47/47 [==============================] - 3s 63ms/step\n",
      "Optimizer: Adamax\n",
      "Test loss: 0.09196841716766357\n",
      "Test accuracy: 0.9749916791915894\n",
      "Precision: 0.9992598\n",
      "Recall: 1.0\n",
      "F-score: 0.9996298\n",
      "\n",
      "\n",
      "47/47 [==============================] - 2s 48ms/step\n",
      "Optimizer: Nadam\n",
      "Test loss: 0.05467596650123596\n",
      "Test accuracy: 0.9819939732551575\n",
      "Precision: 0.9977827\n",
      "Recall: 1.0\n",
      "F-score: 0.9988901\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizers = [tf.keras.optimizers.SGD(),tf.keras.optimizers.Adam(),tf.keras.optimizers.Adadelta(),tf.keras.optimizers.RMSprop(),tf.keras.optimizers.Adagrad(),tf.keras.optimizers.Adamax(),tf.keras.optimizers.Nadam()]\n",
    "\n",
    "# loop through each optimizer and train the model\n",
    "for optimizer in optimizers:\n",
    "    \n",
    "    # reset the model weights for each optimizer\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            keras.Input(shape=(img_height, img_width, 1)),\n",
    "            layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "            layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(128, activation=\"relu\"),\n",
    "            layers.Dense(train_generator.num_classes, activation=\"softmax\"),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # compile the model with the current optimizer\n",
    "    model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    \n",
    "    # train the model\n",
    "    history = model.fit(train_generator, epochs=10, validation_data=test_generator, verbose=0)\n",
    "    \n",
    "    # evaluate the model on test set\n",
    "    test_loss, test_acc = model.evaluate(test_generator, verbose=0)\n",
    "    test_predictions = model.predict(test_generator)\n",
    "    test_pred_labels = np.argmax(test_predictions, axis=1)\n",
    "    test_true_labels = test_generator.classes\n",
    "    \n",
    "    # calculate precision, recall, and F-score\n",
    "    precision = tf.keras.metrics.Precision()(test_true_labels, test_pred_labels)\n",
    "    recall = tf.keras.metrics.Recall()(test_true_labels, test_pred_labels)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    # print the evaluation metrics\n",
    "    print(\"Optimizer:\", optimizer.__class__.__name__)\n",
    "    print(\"Test loss:\", test_loss)\n",
    "    print(\"Test accuracy:\", test_acc)\n",
    "    print(\"Precision:\", precision.numpy())\n",
    "    print(\"Recall:\", recall.numpy())\n",
    "    print(\"F-score:\", f1_score.numpy())\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a9fdfa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 2s 50ms/step\n",
      "Regularizer: L1\n",
      "Test loss: 3.0770576000213623\n",
      "Test accuracy: 0.10003334283828735\n",
      "Precision: 0.9003001\n",
      "Recall: 1.0\n",
      "F-score: 0.9475347\n",
      "\n",
      "\n",
      "47/47 [==============================] - 3s 57ms/step\n",
      "Regularizer: L2\n",
      "Test loss: 0.6127364635467529\n",
      "Test accuracy: 0.9089696407318115\n",
      "Precision: 0.985031\n",
      "Recall: 0.99925923\n",
      "F-score: 0.9920941\n",
      "\n",
      "\n",
      "47/47 [==============================] - 2s 47ms/step\n",
      "Regularizer: L1L2\n",
      "Test loss: 3.0720601081848145\n",
      "Test accuracy: 0.10003334283828735\n",
      "Precision: 0.9003001\n",
      "Recall: 1.0\n",
      "F-score: 0.9475347\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# define a list of regularizers to try\n",
    "regularizers = [regularizers.l1(0.01), regularizers.l2(0.01), regularizers.l1_l2(l1=0.01, l2=0.01)]\n",
    "\n",
    "# loop through each regularizer and train the model\n",
    "for regularizer in regularizers:\n",
    "    \n",
    "    # reset the model weights for each regularizer\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            keras.Input(shape=(img_height, img_width, 1)),\n",
    "            layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\", kernel_regularizer=regularizer),\n",
    "            layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(128, activation=\"relu\", kernel_regularizer=regularizer),\n",
    "            layers.Dense(train_generator.num_classes, activation=\"softmax\"),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # compile the model with default optimizer\n",
    "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    \n",
    "    # train the model\n",
    "    history = model.fit(train_generator, epochs=10, validation_data=test_generator, verbose=0)\n",
    "    \n",
    "    # evaluate the model on test set\n",
    "    test_loss, test_acc = model.evaluate(test_generator, verbose=0)\n",
    "    test_predictions = model.predict(test_generator)\n",
    "    test_pred_labels = np.argmax(test_predictions, axis=1)\n",
    "    test_true_labels = test_generator.classes\n",
    "    \n",
    "    # calculate precision, recall, and F-score\n",
    "    precision = tf.keras.metrics.Precision()(test_true_labels, test_pred_labels)\n",
    "    recall = tf.keras.metrics.Recall()(test_true_labels, test_pred_labels)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    # print the evaluation metrics\n",
    "    print(\"Regularizer:\", regularizer.__class__.__name__)\n",
    "    print(\"Test loss:\", test_loss)\n",
    "    print(\"Test accuracy:\", test_acc)\n",
    "    print(\"Precision:\", precision.numpy())\n",
    "    print(\"Recall:\", recall.numpy())\n",
    "    print(\"F-score:\", f1_score.numpy())\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75d2904f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = 'C:/Users/chand/Desktop/New folder/DeepLearning/Dataset/train'\n",
    "test_data = 'C:/Users/chand/Desktop/New folder/DeepLearning/Dataset/test'\n",
    "train_labels='C:/Users/chand/Desktop/New folder/DeepLearning/Dataset/test'\n",
    "test_labels = 'C:/Users/chand/Desktop/New folder/DeepLearning/Dataset/test'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c78cb328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17000 images belonging to 10 classes.\n",
      "Found 2999 images belonging to 10 classes.\n",
      "Epoch 1/5\n",
      "266/266 [==============================] - 37s 121ms/step - loss: 1.1990 - accuracy: 0.6060 - val_loss: 0.3453 - val_accuracy: 0.9080\n",
      "Epoch 2/5\n",
      "266/266 [==============================] - 31s 115ms/step - loss: 0.5495 - accuracy: 0.8254 - val_loss: 0.1892 - val_accuracy: 0.9503\n",
      "Epoch 3/5\n",
      "266/266 [==============================] - 31s 115ms/step - loss: 0.3683 - accuracy: 0.8861 - val_loss: 0.1484 - val_accuracy: 0.9563\n",
      "Epoch 4/5\n",
      "266/266 [==============================] - 33s 123ms/step - loss: 0.2929 - accuracy: 0.9070 - val_loss: 0.1086 - val_accuracy: 0.9697\n",
      "Epoch 5/5\n",
      "266/266 [==============================] - 34s 127ms/step - loss: 0.2377 - accuracy: 0.9281 - val_loss: 0.0811 - val_accuracy: 0.9783\n",
      "47/47 - 2s - loss: 0.0811 - accuracy: 0.9783 - 2s/epoch - 49ms/step\n",
      "Test accuracy: 0.9783260822296143\n"
     ]
    }
   ],
   "source": [
    "# import necessary libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define the paths to the train and test folders on your desktop\n",
    "train_path = 'C:/Users/chand/Desktop/New folder/DeepLearning/Dataset/train'\n",
    "test_path = 'C:/Users/chand/Desktop/New folder/DeepLearning/Dataset/test'\n",
    "\n",
    "# define image and batch size\n",
    "img_height, img_width = 28, 28\n",
    "batch_size = 64\n",
    "\n",
    "# create ImageDataGenerator objects for data augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255.0,\n",
    "    rotation_range=20,\n",
    "    zoom_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=False,\n",
    "    vertical_flip=False,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255.0\n",
    ")\n",
    "\n",
    "# create flow_from_directory generators for train and test datasets\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    directory=train_path,\n",
    "    target_size=(img_height, img_width),\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    directory=test_path,\n",
    "    target_size=(img_height, img_width),\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=False,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# define the CNN architecture\n",
    "cnn = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(img_height, img_width, 1)),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation=\"relu\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# define the LSTM architecture\n",
    "rnn = keras.Sequential(\n",
    "    [\n",
    "        layers.Reshape((1, 128)),\n",
    "        layers.LSTM(64),\n",
    "        layers.Dense(train_generator.num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# combine the CNN and RNN architectures\n",
    "model = keras.Model(inputs=cnn.input, outputs=rnn(cnn.output))\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# train the model\n",
    "history = model.fit(train_generator, epochs=5, validation_data=test_generator)\n",
    "\n",
    "# evaluate the model on test set\n",
    "test_loss, test_acc = model.evaluate(test_generator, verbose=2)\n",
    "print(\"Test accuracy:\", test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4da2331b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17000 images belonging to 10 classes.\n",
      "Found 2999 images belonging to 10 classes.\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'gradient_tape/categorical_crossentropy/mul/BroadcastGradientArgs' defined at (most recent call last):\n    File \"C:\\Users\\chand\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\chand\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\chand\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\chand\\anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"C:\\Users\\chand\\anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"C:\\Users\\chand\\anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\chand\\anaconda3\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"C:\\Users\\chand\\anaconda3\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"C:\\Users\\chand\\anaconda3\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\chand\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\chand\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\chand\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"C:\\Users\\chand\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\chand\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 390, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"C:\\Users\\chand\\anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\chand\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2914, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\chand\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2960, in _run_cell\n      return runner(coro)\n    File \"C:\\Users\\chand\\anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 78, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\chand\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3185, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\chand\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3377, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"C:\\Users\\chand\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3457, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\chand\\AppData\\Local\\Temp\\ipykernel_10396\\168406048.py\", line 69, in <module>\n      history_cnn = cnn.fit(train_generator, epochs=5, validation_data=test_generator)\n    File \"C:\\Users\\chand\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\chand\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1650, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\chand\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\chand\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1233, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\chand\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1222, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\chand\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1027, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"C:\\Users\\chand\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py\", line 526, in minimize\n      grads_and_vars = self.compute_gradients(loss, var_list, tape)\n    File \"C:\\Users\\chand\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py\", line 259, in compute_gradients\n      grads = tape.gradient(loss, var_list)\nNode: 'gradient_tape/categorical_crossentropy/mul/BroadcastGradientArgs'\nIncompatible shapes: [64,10] vs. [64,128]\n\t [[{{node gradient_tape/categorical_crossentropy/mul/BroadcastGradientArgs}}]] [Op:__inference_train_function_837]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10396\\168406048.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;31m# train the CNN model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m \u001b[0mhistory_cnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_generator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;31m# evaluate the CNN model on test set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'gradient_tape/categorical_crossentropy/mul/BroadcastGradientArgs' defined at (most recent call last):\n    File \"C:\\Users\\chand\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\chand\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\chand\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\chand\\anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"C:\\Users\\chand\\anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"C:\\Users\\chand\\anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\chand\\anaconda3\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"C:\\Users\\chand\\anaconda3\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"C:\\Users\\chand\\anaconda3\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\chand\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\chand\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\chand\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"C:\\Users\\chand\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\chand\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 390, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"C:\\Users\\chand\\anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\chand\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2914, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\chand\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2960, in _run_cell\n      return runner(coro)\n    File \"C:\\Users\\chand\\anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 78, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\chand\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3185, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\chand\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3377, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"C:\\Users\\chand\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3457, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\chand\\AppData\\Local\\Temp\\ipykernel_10396\\168406048.py\", line 69, in <module>\n      history_cnn = cnn.fit(train_generator, epochs=5, validation_data=test_generator)\n    File \"C:\\Users\\chand\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\chand\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1650, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\chand\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\chand\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1233, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\chand\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1222, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\chand\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1027, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"C:\\Users\\chand\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py\", line 526, in minimize\n      grads_and_vars = self.compute_gradients(loss, var_list, tape)\n    File \"C:\\Users\\chand\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py\", line 259, in compute_gradients\n      grads = tape.gradient(loss, var_list)\nNode: 'gradient_tape/categorical_crossentropy/mul/BroadcastGradientArgs'\nIncompatible shapes: [64,10] vs. [64,128]\n\t [[{{node gradient_tape/categorical_crossentropy/mul/BroadcastGradientArgs}}]] [Op:__inference_train_function_837]"
     ]
    }
   ],
   "source": [
    "# import necessary libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# define the paths to the train and test folders on your desktop\n",
    "train_path = 'C:/Users/chand/Desktop/New folder/DeepLearning/Dataset/train'\n",
    "test_path = 'C:/Users/chand/Desktop/New folder/DeepLearning/Dataset/test'\n",
    "\n",
    "# define image and batch size\n",
    "img_height, img_width = 28, 28\n",
    "batch_size = 64\n",
    "\n",
    "# create ImageDataGenerator objects for data augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "rescale=1.0/255.0,\n",
    "rotation_range=20,\n",
    "zoom_range=0.2,\n",
    "shear_range=0.2,\n",
    "width_shift_range=0.2,\n",
    "height_shift_range=0.2,\n",
    "horizontal_flip=False,\n",
    "vertical_flip=False,\n",
    "fill_mode='nearest'\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "rescale=1.0/255.0\n",
    ")\n",
    "\n",
    "# create flow_from_directory generators for train and test datasets\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "directory=train_path,\n",
    "target_size=(img_height, img_width),\n",
    "color_mode=\"grayscale\",\n",
    "batch_size=batch_size,\n",
    "class_mode=\"categorical\",\n",
    "shuffle=True,\n",
    "seed=42\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "directory=test_path,\n",
    "target_size=(img_height, img_width),\n",
    "color_mode=\"grayscale\",\n",
    "batch_size=batch_size,\n",
    "class_mode=\"categorical\",\n",
    "shuffle=False,\n",
    "seed=42\n",
    ")\n",
    "\n",
    "# define the CNN architecture\n",
    "cnn = keras.Sequential(\n",
    "[\n",
    "keras.Input(shape=(img_height, img_width, 1)),\n",
    "layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "layers.Flatten(),\n",
    "layers.Dense(128, activation=\"relu\"),\n",
    "]\n",
    ")\n",
    "\n",
    "# compile the CNN model\n",
    "cnn.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# train the CNN model\n",
    "history_cnn = cnn.fit(train_generator, epochs=5, validation_data=test_generator)\n",
    "\n",
    "# evaluate the CNN model on test set\n",
    "test_loss_cnn, test_acc_cnn = cnn.evaluate(test_generator, verbose=2)\n",
    "print(\"CNN Test accuracy:\", test_acc_cnn)\n",
    "\n",
    "# define the RNN architecture\n",
    "rnn = keras.Sequential(\n",
    "[\n",
    "layers.LSTM(64, input_shape=(1, 128)),\n",
    "layers.Dense(train_generator.num_classes, activation=\"softmax\"),\n",
    "]\n",
    ")\n",
    "\n",
    "# compile the RNN model\n",
    "rnn.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# prepare data for RNN training\n",
    "def extract_features(model, data_generator):\n",
    "    features = model.predict(data_generator)\n",
    "    features = features.reshape((features.shape[0], 1, features.shape[1]))\n",
    "    return features\n",
    "\n",
    "train_features = extract_features(cnn, train_generator)\n",
    "test_features = extract_features(cnn, test_generator)\n",
    "\n",
    "# train the RNN model\n",
    "history_rnn = rnn.fit(train_features, epochs=5, validation_data=(test_features, test_generator.labels))\n",
    "\n",
    "# evaluate the RNN model on test set\n",
    "test_loss_rnn, test_acc_rnn = rnn.evaluate(test_features, test_generator.labels, verbose=2)\n",
    "print(\"RNN Test accuracy:\", test_acc_rnn)\n",
    "\n",
    "# compare the performance of CNN and RNN models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot training and validation accuracy\n",
    "plt.plot(history_cnn.history['accuracy'], label='CNN training accuracy', color='blue')\n",
    "plt.plot(history_cnn.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "306fad8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17000 images belonging to 10 classes.\n",
      "Found 2999 images belonging to 10 classes.\n",
      "Epoch 1/5\n",
      "246/266 [==========================>...] - ETA: 2s - loss: 1.1188 - accuracy: 0.6293"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10396\\2629712356.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;31m# Train the CNN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m \u001b[0mcnn_history\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_generator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;31m# Train the RNN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1648\u001b[0m                         ):\n\u001b[0;32m   1649\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1650\u001b[1;33m                             \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1651\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1652\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    910\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    911\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 912\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    913\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    132\u001b[0m       (concrete_function,\n\u001b[0;32m    133\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m--> 134\u001b[1;33m     return concrete_function._call_flat(\n\u001b[0m\u001b[0;32m    135\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1744\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1745\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    376\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 378\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    379\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define paths to the train and test folders\n",
    "train_path = 'C:/Users/chand/Desktop/New folder/DeepLearning/Dataset/train'\n",
    "test_path = 'C:/Users/chand/Desktop/New folder/DeepLearning/Dataset/test'\n",
    "\n",
    "# Define image and batch size\n",
    "img_height, img_width = 28, 28\n",
    "batch_size = 64\n",
    "\n",
    "# Create ImageDataGenerator objects for data augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255.0,\n",
    "    rotation_range=20,\n",
    "    zoom_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=False,\n",
    "    vertical_flip=False,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255.0\n",
    ")\n",
    "\n",
    "# Create flow_from_directory generators for train and test datasets\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    directory=train_path,\n",
    "    target_size=(img_height, img_width),\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    directory=test_path,\n",
    "    target_size=(img_height, img_width),\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=batch_size,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=False,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Define the CNN architecture\n",
    "cnn = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(img_height, img_width, 1)),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation=\"relu\"),\n",
    "        layers.Dense(train_generator.num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define the LSTM architecture\n",
    "rnn = keras.Sequential(\n",
    "    [\n",
    "        layers.Reshape((img_height, img_width)),\n",
    "        layers.Bidirectional(layers.LSTM(64)),\n",
    "        layers.Dense(train_generator.num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Compile the models\n",
    "cnn.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "rnn.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the CNN\n",
    "cnn_history = cnn.fit(train_generator, epochs=5, validation_data=test_generator)\n",
    "\n",
    "# Train the RNN\n",
    "rnn_history = rnn.fit(train_generator, epochs=5, validation_data=test_generator)\n",
    "\n",
    "# Evaluate the models on test set\n",
    "cnn_test_loss, cnn_test_acc = cnn.evaluate(test_generator, verbose=2)\n",
    "rnn_test_loss, rnn_test_acc = rnn.evaluate(test_generator, verbose=2)\n",
    "\n",
    "print(\"CNN Test accuracy:\", cnn_test_acc)\n",
    "print(\"RNN Test accuracy:\", rnn_test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b323fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
